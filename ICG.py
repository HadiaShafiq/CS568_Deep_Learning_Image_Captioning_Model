# -*- coding: utf-8 -*-
"""ICG Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vAdTZr-C-XdlGA1cA_L1AuQktCqYOVn8

# **Image Caption Generator**

**Dataset**: Flicker8k

**Feature Extraction Model**: Inception V3

**Model**: LSTM

**Epochs**: 5

**Batch size**: 64
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import keras
import os

print('tensorflow: %s' % tf.__version__)
print('keras: %s' % keras.__version__)

"""# Loading the Dataset """

dataset = "/content/drive/MyDrive/Dataset"

## The location of the Flickr8K_ images
dataset_images = dataset + "/Flickr8k_Dataset/Flicker8k_Dataset"


## The location of the caption file
dataset_captions = dataset + "/Flickr8k_text/Flickr8k.token.txt"

imgs = os.listdir(dataset_images)
print("The number of jpg flies in Flicker8k: {}".format(len(imgs)))

captions_file = open(dataset_captions, 'r')
captions = captions_file.read()
captions_file.close()

captions

for line in captions.split('\n'):
  print(line[:50])

"""# Creating a Dcitionary with Image Id as keys and Image Captions as values"""

descriptions = dict()
for line in captions.split('\n'):
    # split line by white space
    tokens = line.split()
    if len(line) < 2:
      continue
    image_id, image_desc = tokens[0], tokens[1:]
    
    # get the filename from image id
    image_id = image_id.split('.')[0]
    
    image_desc = ' '.join(image_desc)
    if image_id not in descriptions:
        descriptions[image_id] = list()
    descriptions[image_id].append(image_desc)

"""### Plotting Some images with their captions"""

desc = list(descriptions.items())[:5]

for val in desc:
  print(val)

from keras.preprocessing.image import load_img, img_to_array
from IPython.display import display
from PIL import Image
import matplotlib.pyplot as plt

no_of_imgs = 5  # Displaying 5 images from the dataset
no_of_pixels = 224
target_size = (no_of_pixels, no_of_pixels, 3)

count = 1 
fig = plt.figure(figsize=(10,20))

for val in desc:
    image_name = dataset_images  + '/' + val[0] + '.jpg' 
    image_captions = val[1]
    
    image_load = load_img(image_name, target_size=target_size)
    ax = fig.add_subplot(no_of_imgs, 2, count, xticks=[], yticks=[])
    ax.imshow(image_load)
    count += 1
    
    ax = fig.add_subplot(no_of_imgs, 2 , count)
    plt.axis('off')
    ax.plot()
    ax.set_xlim(0,1)
    ax.set_ylim(0, len(image_captions))
    for i, image_captions in enumerate(image_captions):
        ax.text(0, i, image_captions, fontsize=16)
    count += 1
plt.show()

"""# Loading the Training Images"""

img_id = list(descriptions.keys())

def addTokens(captions):
  tokenized_caption = list()
  for i in range(len(captions)):
    tokenized_caption.append('<START>' + captions[i] + '<END>')
  return tokenized_caption

print(addTokens(descriptions['3183060123_ea3af6278b']))

training_images_id_txtFile =  dataset + "/Flickr8k_text/Flickr_8k.trainImages.txt"
training_images_desc = dict()
tid = list()

tfile = open(training_images_id_txtFile, 'r')
training_images_id = tfile.read()
tfile.close()

for line in training_images_id.split('\n'):
  id = line.split('.')[0]
  print(id)

"""# Loading the Testing Images"""

testing_images_id_txtFile =  dataset + "/Flickr8k_text/Flickr_8k.testImages.txt"
testing_images_desc = dict()
testid = list()

tfile = open(testing_images_id_txtFile, 'r')
testing_img_id = tfile.read()
tfile.close()

testing_images_id = list()
for line in testing_img_id.split('\n'):
  testing_images_id.append(line.split('.')[0])

testing_images_id = ' '.join(testing_images_id).split()

"""## Adding Tokens to training images caption"""

for line in training_images_id.split('\n'):
  id = line.split('.')[0]
  if id in training_images_id:
    if id!='' and id not in tid :
      tc = addTokens(descriptions[id])
      tid.append(id)
      training_images_desc[id] = tc

training_images_desc['3107889179_106d223345']

"""# Extracting Features

"""

from tensorflow.keras.applications import InceptionV3

training_img_ids = list(training_images_desc.keys())
img_path = training_img_ids.copy()
for i in range(len(training_img_ids)):
    img_path[i] = dataset_images  + '/' + img_path[i] + '.jpg'

len(training_img_ids)

img_path[:5]

model = InceptionV3(weights='imagenet')

# excluding last layer

image_features_extract_model = tf.keras.Model(model.input, model.layers[-2].output )

image_features_extract_model.summary()

import numpy as np
from keras.preprocessing import image
from keras.applications.inception_v3 import preprocess_input

def preProcessingImages(id):
  impath = dataset + "/Flickr8k_Dataset/Flicker8k_Dataset" + "/" + id + ".jpg"
  img =  image.load_img(impath, target_size=(299, 299))
  x = image.img_to_array(img)
  # Add one more dimension
  x = np.expand_dims(x, axis=0)
  # preprocess the images using preprocess_input() from inception module
  x = preprocess_input(x)
  return x

def img_encoding(id):
    image = preProcessingImages(id)
    feature_vector = image_features_extract_model.predict(image) # Get the encoding vector for the image
    feature_vector = np.reshape(feature_vector, feature_vector.shape[1]) # reshape from (1, 2048) to (2048, )
    return feature_vector

training_img_encoding = {}
for id in training_img_ids:
  #training_img_encoding[id] = img_encoding(id)
  training_img_encoding[id[len(img_id):]] = img_encoding(id)

from pickle import dump, load
import pickle

with open( dataset + "/training_encoded_images.pkl", "wb") as training_encoded_images_pkl:
  pickle.dump(training_img_encoding, training_encoded_images_pkl)

training_img_features = load(open(dataset + "/training_encoded_images.pkl", "rb"))

testing_img_encoding = {}
for id in testing_images_id:
  testing_img_encoding[id[len(img_id):]] = img_encoding(id)

with open( dataset + "/testing_encoded_images.pkl", "wb") as testing_encoded_images_pkl:
  pickle.dump(testing_img_encoding, testing_encoded_images_pkl)

"""# Tokenizing Captions: Text to Numbers """

captions_img = list(descriptions.values())

#all_captions = list(np.concatenate(captions_img). flat)

# tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(captions_img)
vocab_size = len(tokenizer.word_index) + 1

vocab_size

from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical, plot_model

# create data generator to get data in batch 
def data_generator(descriptions, features, tokenizer, max_length, batch_size):
  X1, X2, y = list(), list(), list()
  n=0
  for i in range(batch_size):
    for id, captions in descriptions.items():
      n = n + 1
      #retrieve feature of image
      img_feature = features[id]
      # process each caption in the captions list
      for caption in captions_img:
        # encode the sequence
        seq = tokenizer.texts_to_sequences([caption])[0]
        # split the sequence into multiple X, y pairs
        for i in range(1, len(seq)):
          # split into input and output pairs
          in_seq, out_seq = seq[:i], seq[i]
          # pad input sequence
          in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
          # encode output sequence
          out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
          
          # store the sequences
          X1.append(img_feature)
          X2.append(in_seq)
          y.append(out_seq)
          if n == batch_size:
            yield [[array(X1), array(X2)], array(y)]
            X1, X2, y = list(), list(), list()
            n = 0

from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add

# get maximum length of the caption available
max_length = max(len(caption.split()) for caption in all_captions)
max_length

epochs = 5
batch_size = 32
steps = len(training_images_id) // batch_size

for i in range(epochs):
    # create data generator
    generator = data_generator(training_images_desc, training_img_features, tokenizer, max_length, batch_size)
    # fit for one epoch
    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)

# image feature layers
inputs1 = Input(shape=(4096,))
fe1 = Dropout(0.4)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
# sequence feature layers
inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.4)(se1)
se3 = LSTM(256)(se2)

# decoder model
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

model =  tf.keras.Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')

# plot the model
plot_model(model, show_shapes=True)





